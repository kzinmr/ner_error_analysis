{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base\n",
      "micro avg {'p': ' 0.764', 'r': '0.762', 'f1': '0.763', 's': '12721'}\n",
      "macro avg {'p': ' 0.674', 'r': '0.559', 'f1': '0.597', 's': '12721'}\n",
      "wmicro avg {'p': ' 0.763', 'r': '0.762', 'f1': '0.760', 's': '12721'}\n",
      "kb_ohe\n",
      "micro avg {'p': ' 0.755', 'r': '0.758', 'f1': '0.757', 's': '12721'}\n",
      "macro avg {'p': ' 0.666', 'r': '0.547', 'f1': '0.579', 's': '12721'}\n",
      "wmicro avg {'p': ' 0.757', 'r': '0.758', 'f1': '0.754', 's': '12721'}\n",
      "kb_ce\n",
      "micro avg {'p': ' 0.749', 'r': '0.757', 'f1': '0.753', 's': '12721'}\n",
      "macro avg {'p': ' 0.647', 'r': '0.564', 'f1': '0.589', 's': '12721'}\n",
      "wmicro avg {'p': ' 0.747', 'r': '0.757', 'f1': '0.749', 's': '12721'}\n"
     ]
    }
   ],
   "source": [
    "def read_classification_report(filename):\n",
    "    with open(filename) as f:\n",
    "        s = f.read()\n",
    "    lines = [ss.strip().split('     ') for ss in s.split('\\n') if ss]\n",
    "    micro = []\n",
    "    macro = []\n",
    "    wmicro = []\n",
    "    for i, line in enumerate(lines):\n",
    "        if i % 3 == 0:\n",
    "            micro.append({'p': line[1], 'r': line[2], 'f1': line[3], 's': line[4]})\n",
    "        elif i % 3 == 1:\n",
    "            macro.append({'p': line[1], 'r': line[2], 'f1': line[3], 's': line[4]})\n",
    "        else:\n",
    "            wmicro.append({'p': line[1], 'r': line[2], 'f1': line[3], 's': line[4]})\n",
    "    return {'micro': micro[-1], 'macro': macro[-1], 'wmicro': wmicro[-1]}\n",
    "\n",
    "d = read_classification_report('classification_base_ep9.txt')\n",
    "print('base')\n",
    "print('micro avg', d['micro'])\n",
    "print('macro avg', d['macro'])\n",
    "print('wmicro avg', d['wmicro'])\n",
    "\n",
    "d = read_classification_report('classification_kb_ohe_ep9.txt')\n",
    "print('kb_ohe')\n",
    "print('micro avg', d['micro'])\n",
    "print('macro avg', d['macro'])\n",
    "print('wmicro avg', d['wmicro'])\n",
    "\n",
    "d = read_classification_report('classification_kb_ce_ep9.txt')\n",
    "print('kb_ce')\n",
    "print('micro avg', d['micro'])\n",
    "print('macro avg', d['macro'])\n",
    "print('wmicro avg', d['wmicro'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro avg {'p': '0.846', 'r': '0.667', 'f1': '0.746', 's': '12852'}\n",
      "macro avg {'p': '0.768', 'r': '0.476', 'f1': '0.567', 's': '12852'}\n",
      "weighted avg {'p': '0.842', 'r': '0.667', 'f1': '0.736', 's': '12852'}\n"
     ]
    }
   ],
   "source": [
    "with open('crf_outputs/classification_report_crf_beam_fix.txt') as f:\n",
    "    for l in [[ll.strip() for ll in l.split('     ') if ll] for l in f.readlines() if l.strip()][-3:]:\n",
    "        print(l[0], {'p': l[1], 'r': l[2], 'f1': l[3], 's': l[4]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def select(x, k=10):\n",
    "    return random.sample(x, k)\n",
    "\n",
    "def select_dict(x, k=10):\n",
    "    return sorted(x.items(), key=lambda x: -x[1])[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 'ant'), (1, 'bird'), (2, 'cat')]\n",
      "gold -> pred : support\n",
      "bird -> cat : 1\n",
      "cat -> ant : 1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_true = [\"cat\", \"ant\", \"cat\", \"cat\", \"ant\", \"bird\"]\n",
    "y_pred = [\"ant\", \"ant\", \"cat\", \"cat\", \"ant\", \"cat\"]\n",
    "LABELS = [\"ant\", \"bird\", \"cat\"]\n",
    "print(list(enumerate(LABELS)))\n",
    "\n",
    "def show_confusion(y_true, y_pred, LABELS, min_count=0):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=LABELS)  # axis=(gold, pred)\n",
    "    print('gold -> pred : support')\n",
    "    for gold_i, a in enumerate(cm):\n",
    "        for pred_i, c in enumerate(a):\n",
    "            if gold_i != pred_i and c > min_count:\n",
    "                print(f'{LABELS[gold_i]} -> {LABELS[pred_i]} : {c}')\n",
    "show_confusion(y_true, y_pred, LABELS, min_count=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def read_token_label_list(filename):\n",
    "    with open(filename) as f:\n",
    "        sentences = [[l.split('\\t') for l in s.split('\\n') if len(l.split('\\t')) == 3] for s in f.read().split('\\n\\n')]\n",
    "    print(len(sentences))\n",
    "    sentences_err = [s for s in sentences if any(pred != gold for surf, pred, gold in s)]\n",
    "    print(len(sentences_err))\n",
    "    sentences_pos = [s for s in sentences if all(pred == gold for surf, pred, gold in s)]\n",
    "    return sentences_err, sentences_pos\n",
    "\n",
    "def categorize_errors(sentences_err):\n",
    "    fns = []\n",
    "    fps_o = []\n",
    "    fps_conf = []\n",
    "\n",
    "    for i, s in enumerate(sentences_err):\n",
    "        sent = '\\n'.join( ' '.join(line) for line in s)\n",
    "    #     print(sent)\n",
    "        for surf, pred, gold in s:\n",
    "            if pred == 'O' and gold != 'O':  # gold != 'O'; FN(ひろいこぼし)\n",
    "                fns.append({'target':(surf, pred, gold), 'sentence': sent , 'sid': i})\n",
    "            elif pred != 'O':  # FP\n",
    "                if gold == 'O':\n",
    "                    fps_o.append({'target':(surf, pred, gold), 'sentence': sent , 'sid': i})\n",
    "                elif pred != gold:  # gold != 'O'\n",
    "                    fps_conf.append({'target':(surf, pred, gold), 'sentence': sent , 'sid': i})\n",
    "    print(len(sentences_err), len(fns), len(fps_o), len(fps_conf))\n",
    "    return fns, fps_o, fps_conf\n",
    "\n",
    "\n",
    "def categorize_errors_detail(fns, fps_conf, fps_o):\n",
    "\n",
    "    # FN\n",
    "    fns_counter = Counter(d['target'][2].split('-')[1] for d in fns)\n",
    "    fns_entries = {d['target'][2].split('-')[1]: [] for d in fns}\n",
    "    fns_uniq = {d['sid']: [] for d in fns}\n",
    "    for d in fns:\n",
    "        fns_uniq[ d['sid'] ].append(d['target'])\n",
    "        fns_entries[ d['target'][2].split('-')[1] ].append(d['target'])\n",
    "\n",
    "    # FP: confusion\n",
    "    fps_conf_g_counter = Counter(d['target'][2].split('-')[1] for d in fps_conf)\n",
    "    fps_conf_g_entries = {d['target'][2].split('-')[1]: [] for d in fps_conf}\n",
    "    fps_conf_p_counter = Counter(d['target'][1].split('-')[1] for d in fps_conf if d['target'][1] != 'X')\n",
    "    fps_conf_p_entries = {d['target'][1].split('-')[1]: [] for d in fps_conf if d['target'][1] != 'X'}\n",
    "    fps_conf_uniq = {d['sid']: [] for d in fps_conf}\n",
    "\n",
    "    y_gold, y_pred = [], []\n",
    "    labels = set()\n",
    "    fps_confusion = {(d['target'][2], d['target'][1]): [] for d in fps_conf}\n",
    "\n",
    "    for d in fps_conf:\n",
    "        fps_conf_uniq[ d['sid'] ].append(d['target'])\n",
    "        fps_conf_g_entries[ d['target'][2].split('-')[1] ].append(d['target'])\n",
    "        if d['target'][1] != 'X':\n",
    "            fps_conf_p_entries[ d['target'][1].split('-')[1] ].append(d['target'])\n",
    "            y_gold.append(d['target'][2])\n",
    "            y_pred.append(d['target'][1])\n",
    "            labels.add(d['target'][2])\n",
    "            labels.add(d['target'][1])\n",
    "            fps_confusion[(d['target'][2], d['target'][1])].append(d['target'])\n",
    "    labels = sorted(labels, key=lambda x: x.split('-')[1]+x[0].split('-')[0])\n",
    "\n",
    "    # FP: O as positive\n",
    "    fps_o_counter = Counter(d['target'][1] for d in fps_o)  # .split('-')[1]\n",
    "    fps_o_entries = {d['target'][1]: [] for d in fps_o}  # .split('-')[1]\n",
    "    fps_o_uniq = {d['sid']: [] for d in fps_o}\n",
    "    for d in fps_o:\n",
    "        fps_o_uniq[ d['sid'] ].append(d['target'])\n",
    "        fps_o_entries[ d['target'][1] ].append(d['target'])  # .split('-')[1]\n",
    "\n",
    "    return {\n",
    "'fn': {\n",
    "'sid2result': fns_uniq,\n",
    "'counter_gold': fns_counter,\n",
    "'entry_map_gold': fns_entries,\n",
    "},\n",
    "\n",
    "'fp_confusion': {\n",
    "'sid2result': fps_conf_uniq,\n",
    "'counter_gold': fps_conf_g_counter, 'counter_pred': fps_conf_p_counter,\n",
    "'entry_map_gold': fps_conf_g_entries, 'entry_map_pred': fps_conf_p_entries,\n",
    "'confusion_info': {'y_gold': y_gold, 'y_pred': y_pred, 'labels': labels, 'confusion_map': fps_confusion}\n",
    "},\n",
    "    \n",
    "'fp_o': {\n",
    "'sid2result': fps_o_uniq,\n",
    "'counter_pred': fps_o_counter,\n",
    "'entry_map_pred': fps_o_entries,\n",
    "}\n",
    "    }\n",
    "\n",
    "\n",
    "def error_analysis(filename):\n",
    "    sentences_err, sentences_pos = read_token_label_list(filename)\n",
    "    fns, fps_o, fps_conf = categorize_errors(sentences_err)\n",
    "    return categorize_errors_detail(fns, fps_conf, fps_o)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3369\n",
      "1559\n",
      "3374\n",
      "1601\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def read_token_label_list(filename):\n",
    "    with open(filename) as f:\n",
    "        sentences = [[l.split('\\t') for l in s.split('\\n') if len(l.split('\\t')) == 3] for s in f.read().split('\\n\\n')]\n",
    "    print(len(sentences))\n",
    "    sentences_err = [s for s in sentences if any(pred != gold for surf, pred, gold in s)]\n",
    "    print(len(sentences_err))\n",
    "    sentences_pos = [s for s in sentences if all(pred == gold for surf, pred, gold in s)]\n",
    "    return sentences_err, sentences_pos\n",
    "\n",
    "filename = 'output_result_base_ep9/token_label_list_epoch9_beam_fix.txt'\n",
    "sentences_err, sentences_pos = read_token_label_list(filename)\n",
    "\n",
    "filename = 'crf_outputs/token_label_pred_crf_beam_fix.txt'\n",
    "sentences_err_crf, sentences_pos_crf = read_token_label_list(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def extract_chunk(sentence, from_gold=True):\n",
    "    chunks = []\n",
    "    chunk = []\n",
    "    for surf, pred, gold in sentence:\n",
    "        if from_gold:\n",
    "            if '-' in gold:\n",
    "                bio, netype = gold.split('-')\n",
    "                if bio == 'B':\n",
    "                    chunk = [(surf, pred, gold)]\n",
    "                elif bio == 'I':\n",
    "                    chunk.append((surf, pred, gold))\n",
    "            elif chunk:\n",
    "                chunks.append(chunk)\n",
    "                chunk = []\n",
    "        else:\n",
    "            if '-' in pred:\n",
    "                bio, netype = pred.split('-')\n",
    "                if bio == 'B':\n",
    "                    chunk = [(surf, pred, gold)]\n",
    "                elif bio == 'I':\n",
    "                    chunk.append((surf, pred, gold))\n",
    "            elif chunk:\n",
    "                chunks.append(chunk)\n",
    "                chunk = []\n",
    "    if chunk:\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_exact_match(chunk):\n",
    "    return all(pred == gold for _, pred, gold in chunk)\n",
    "\n",
    "def check_chunks_match(chunks_pos, chunks_err_gold, chunks_err_pred):\n",
    "    tp = 0\n",
    "    for chunks in chunks_pos:\n",
    "        for chunk in chunks:\n",
    "            assert is_exact_match(chunk)\n",
    "            tp += 1\n",
    "        # sentence\n",
    "    for chunks in chunks_err_gold:\n",
    "        for chunk in chunks:\n",
    "            if is_exact_match(chunk):\n",
    "                tp += 1\n",
    "\n",
    "    fn = 0\n",
    "    for chunks in chunks_err_gold:\n",
    "        for chunk in chunks:\n",
    "            if not is_exact_match(chunk):\n",
    "                fn += 1\n",
    "\n",
    "    fp = 0\n",
    "    for chunks in chunks_err_pred:\n",
    "        for chunk in chunks:\n",
    "            if not is_exact_match(chunk):\n",
    "                fp += 1\n",
    "\n",
    "    p = tp / (tp + fp)\n",
    "    r = tp / (tp + fn)\n",
    "    f1 = 2 / (1 / p + 1 / r)\n",
    "    return {'TP': tp, 'FP': fp, 'FN': fn, 'P': p, 'R': r, 'F1': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chunks_pos = [extract_chunk(sentence) for sentence in sentences_pos]\n",
    "chunks_err_gold = [extract_chunk(sentence) for sentence in sentences_err]\n",
    "chunks_err_pred = [extract_chunk(sentence, from_gold=False) for sentence in sentences_err]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chunks_pos_crf = [extract_chunk(sentence) for sentence in sentences_pos_crf]\n",
    "chunks_err_gold_crf = [extract_chunk(sentence) for sentence in sentences_err_crf]\n",
    "chunks_err_pred_crf = [extract_chunk(sentence, from_gold=False) for sentence in sentences_err_crf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TP': 5166,\n",
       " 'FP': 1590,\n",
       " 'FN': 1663,\n",
       " 'P': 0.7646536412078153,\n",
       " 'R': 0.7564797188460975,\n",
       " 'F1': 0.7605447184394553}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_chunks_match(chunks_pos, chunks_err_gold, chunks_err_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TP': 4713,\n",
       " 'FP': 752,\n",
       " 'FN': 2238,\n",
       " 'P': 0.8623970722781336,\n",
       " 'R': 0.678031937850669,\n",
       " 'F1': 0.7591817010309277}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_chunks_match(chunks_pos_crf, chunks_err_gold_crf, chunks_err_pred_crf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_error_detail(chunks_err_gold):\n",
    "    fn_partial = 0  # 部分一致誤り\n",
    "    fn_confusion = 0  # クラス誤り\n",
    "    fn_o = 0  # 未抽出誤り\n",
    "    rest = []\n",
    "    for chunks in chunks_err_gold:\n",
    "        for chunk in chunks:\n",
    "            if any(pred.split('-')[-1] == gold.split('-')[-1] for _, pred, gold in chunk):\n",
    "                fn_partial += 1\n",
    "            elif all(pred == 'O' for _, pred, gold in chunk):\n",
    "                fn_o += 1\n",
    "            else:\n",
    "                fn_confusion += 1\n",
    "                rest.append(chunk)\n",
    "    return {'partial': fn_partial, 'confusion': fn_confusion, 'O': fn_o}, rest  # {'partial': 2882, 'confusion': 499, 'O': 785}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'partial': 2921, 'confusion': 403, 'O': 772}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result, rest = chunk_error_detail(chunks_err_gold)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'partial': 2336, 'confusion': 243, 'O': 1723}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_crf, rest_crf = chunk_error_detail(chunks_err_gold_crf)\n",
    "result_crf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('京都', 'B-City', 'B-Province')],\n",
       " [('小', 'B-Person', 'B-City'), ('鹿田', 'O', 'I-City')],\n",
       " [('葛屋', 'B-Ethnic_Group_Other', 'B-Position_Vocation')],\n",
       " [('本長', 'B-Person', 'B-Worship_Place'),\n",
       "  ('谷寺', 'I-Person', 'I-Worship_Place')],\n",
       " [('日本', 'B-Country', 'B-Organization_Other'),\n",
       "  ('三', 'O', 'I-Organization_Other'),\n",
       "  ('大', 'O', 'I-Organization_Other'),\n",
       "  ('荒神', 'O', 'I-Organization_Other')],\n",
       " [('笠', 'B-GOE_Other', 'B-City')],\n",
       " [('笠', 'B-GOE_Other', 'B-City')],\n",
       " [('京都', 'B-City', 'B-Province')],\n",
       " [('道路', 'B-Corporation_Other', 'B-Government'),\n",
       "  ('公団', 'O', 'I-Government'),\n",
       "  ('民営', 'O', 'I-Government'),\n",
       "  ('化', 'O', 'I-Government'),\n",
       "  ('委員', 'O', 'I-Government'),\n",
       "  ('会', 'O', 'I-Government')],\n",
       " [('日本', 'B-Nationality', 'B-Country')]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rest[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('京都', 'B-City', 'B-Province')],\n",
       " [('京都', 'B-City', 'B-Province')],\n",
       " [('道路', 'B-Position_Vocation', 'B-Government'),\n",
       "  ('公団', 'I-Position_Vocation', 'I-Government'),\n",
       "  ('民営', 'I-Position_Vocation', 'I-Government'),\n",
       "  ('化', 'I-Position_Vocation', 'I-Government'),\n",
       "  ('委員', 'I-Position_Vocation', 'I-Government'),\n",
       "  ('会', 'O', 'I-Government')],\n",
       " [('学習', 'B-Person', 'B-School'), ('院', 'I-Person', 'I-School')],\n",
       " [('宮内', 'B-Position_Vocation', 'B-Government'),\n",
       "  ('庁', 'I-Position_Vocation', 'I-Government')],\n",
       " [('宮内', 'B-Government', 'B-Position_Vocation'),\n",
       "  ('庁', 'I-Government', 'I-Position_Vocation'),\n",
       "  ('ＯＢ', 'O', 'I-Position_Vocation')],\n",
       " [('サッカー', 'B-Sports_Organization_Other', 'B-Position_Vocation'),\n",
       "  ('日本', 'I-Sports_Organization_Other', 'I-Position_Vocation'),\n",
       "  ('代表', 'I-Sports_Organization_Other', 'I-Position_Vocation')],\n",
       " [('大阪', 'B-City', 'B-Province')],\n",
       " [('マルコ', 'B-Company', 'B-Person'), ('ポーロ', 'I-Company', 'I-Person')],\n",
       " [('日', 'B-Person', 'B-Country'), ('朝', 'I-Person', 'I-Country')]]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rest_crf[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3369\n",
      "1566\n",
      "1566 2217 1578 1120\n"
     ]
    }
   ],
   "source": [
    "\n",
    "error_detail = error_analysis('output_result_base_ep9/token_label_list_epoch9_greedy_fix.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(886,\n",
       " [('Position_Vocation', 566),\n",
       "  ('Person', 289),\n",
       "  ('Company', 161),\n",
       "  ('Corporation_Other', 143),\n",
       "  ('Government', 118),\n",
       "  ('Organization_Other', 112),\n",
       "  ('GOE_Other', 107),\n",
       "  ('City', 102),\n",
       "  ('International_Organization', 84),\n",
       "  ('Country', 57)])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 未抽出誤り; 知識依存\n",
    "fn = error_detail['fn']\n",
    "len(fn['sid2result']), select_dict(fn['counter_gold'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(637,\n",
       " [('Position_Vocation', 257),\n",
       "  ('Person', 106),\n",
       "  ('Company', 83),\n",
       "  ('Government', 79),\n",
       "  ('City', 67),\n",
       "  ('GOE_Other', 53),\n",
       "  ('Corporation_Other', 51),\n",
       "  ('Organization_Other', 47),\n",
       "  ('Public_Institution', 31),\n",
       "  ('Sports_Organization_Other', 30)],\n",
       " [('Position_Vocation', 224),\n",
       "  ('Person', 163),\n",
       "  ('GOE_Other', 85),\n",
       "  ('Corporation_Other', 84),\n",
       "  ('Government', 84),\n",
       "  ('City', 80),\n",
       "  ('Company', 74),\n",
       "  ('Public_Institution', 37),\n",
       "  ('Country', 36),\n",
       "  ('Province', 32)])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FP混同; 文脈・語彙の問題?; KB的に対処可能\n",
    "# - 正解との部分一致: 処理が面倒（一致するところも保持）\n",
    "# - クラス誤り\n",
    "\n",
    "fp_confusion = error_detail['fp_confusion']\n",
    "len(fp_confusion['sid2result']), select_dict(fp_confusion['counter_gold']), select_dict(fp_confusion['counter_pred'])\n",
    "#x[0].split('-')[1]+x[0].split('-')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(799,\n",
       " [('B-Position_Vocation', 244),\n",
       "  ('B-Person', 213),\n",
       "  ('I-Position_Vocation', 182),\n",
       "  ('X', 177),\n",
       "  ('I-Person', 76),\n",
       "  ('B-City', 69),\n",
       "  ('I-GOE_Other', 59),\n",
       "  ('B-Country', 57),\n",
       "  ('B-Company', 56),\n",
       "  ('B-GOE_Other', 40)])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FP; 過抽出誤り; 文脈・語彙の問題?\n",
    "\n",
    "fp_o = error_detail['fp_o']\n",
    "len(fp_o['sid2result']), select_dict(fp_o['counter_pred'])\n",
    "#x[0].split('-')[1]+x[0].split('-')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2217, 1120)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FN errors\n",
    "sum([v for k, v in fn['counter_gold'].items()]), sum(v for k, v in fp_confusion['counter_gold'].items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1105, 1578)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FP error\n",
    "sum([v for k, v in fp_confusion['counter_pred'].items()]), sum(v for k, v in fp_o['counter_pred'].items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function sklearn_crfsuite.metrics.flat_classification_report(y_true, y_pred, labels=None, **kwargs)>"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "metrics.flat_classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FN: 未抽出誤り\n",
    "- 知識ベースで改善する見込みがある"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select(list(fn['entry_map_gold'].items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('[UNK]', 'O', 'B-Person'),\n",
       " ('パウンド', 'O', 'B-Person'),\n",
       " ('ガトームソン', 'O', 'B-Person'),\n",
       " ('Ｐ', 'O', 'I-Person'),\n",
       " ('竹蔵', 'O', 'B-Person'),\n",
       " ('美紗', 'O', 'B-Person'),\n",
       " ('\\u3000', 'O', 'I-Person'),\n",
       " ('トルシエ', 'O', 'B-Person'),\n",
       " ('サザエ', 'O', 'B-Person'),\n",
       " ('文公', 'O', 'B-Person')]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select(fn['entry_map_gold']['Person'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FP混同\n",
    "## - 正解との部分一致\n",
    "## - クラス誤り"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select(list(fp_confusion['entry_map_gold'].items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('武寿', 'B-City', 'B-Person'),\n",
       " ('家宝', 'B-Person', 'I-Person'),\n",
       " ('赤松', 'B-Family', 'B-Person'),\n",
       " ('シー', 'I-Position_Vocation', 'B-Person'),\n",
       " ('新太', 'B-Person', 'I-Person'),\n",
       " ('教夫', 'B-Person', 'I-Person'),\n",
       " ('深町', 'B-City', 'B-Person'),\n",
       " ('高崎', 'B-Position_Vocation', 'B-Person'),\n",
       " ('新渡戸', 'B-Station', 'B-Person'),\n",
       " ('信雄', 'B-Person', 'I-Person')]"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select(fp_confusion['entry_map_gold']['Person'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gold -> pred : support\n",
      "B-City -> B-GOE_Other : 11\n",
      "B-City -> B-Person : 14\n",
      "B-City -> B-Province : 13\n",
      "B-Company -> B-Corporation_Other : 7\n",
      "B-Company -> B-GOE_Other : 9\n",
      "B-Company -> B-Person : 9\n",
      "I-Company -> B-Company : 6\n",
      "I-Company -> I-GOE_Other : 9\n",
      "B-Company_Group -> B-Company : 7\n",
      "I-Corporation_Other -> B-Corporation_Other : 6\n",
      "I-Corporation_Other -> I-Government : 18\n",
      "B-Country -> B-Nationality : 6\n",
      "B-Family -> B-Person : 6\n",
      "B-GOE_Other -> B-City : 11\n",
      "I-GOE_Other -> B-GOE_Other : 8\n",
      "B-GPE_Other -> B-City : 6\n",
      "B-Government -> B-Corporation_Other : 8\n",
      "B-Government -> I-Government : 9\n",
      "B-Government -> B-Position_Vocation : 10\n",
      "I-Government -> I-Corporation_Other : 20\n",
      "I-Government -> I-Position_Vocation : 6\n",
      "I-Organization_Other -> I-Corporation_Other : 6\n",
      "I-Organization_Other -> I-Government : 7\n",
      "B-Person -> B-City : 13\n",
      "B-Person -> B-Company : 10\n",
      "B-Person -> I-Person : 9\n",
      "B-Person -> B-Position_Vocation : 7\n",
      "I-Person -> B-Person : 28\n",
      "B-Position_Vocation -> B-Person : 20\n",
      "B-Position_Vocation -> I-Person : 6\n",
      "B-Position_Vocation -> I-Position_Vocation : 45\n",
      "I-Position_Vocation -> I-Government : 6\n",
      "I-Position_Vocation -> I-International_Organization : 6\n",
      "I-Position_Vocation -> I-Organization_Other : 7\n",
      "I-Position_Vocation -> B-Position_Vocation : 93\n",
      "B-Province -> B-City : 6\n",
      "I-Public_Institution -> I-Government : 6\n",
      "I-Research_Institute -> I-Corporation_Other : 9\n"
     ]
    }
   ],
   "source": [
    "# 'confusion_info': {'y_gold': y_gold, 'y_pred': y_pred, 'labels': labels, 'confusion_map': fps_confusion}\n",
    "confusion_info = fp_confusion['confusion_info']\n",
    "y_gold, y_pred, labels = confusion_info['y_gold'], confusion_info['y_pred'], confusion_info['labels'], \n",
    "\n",
    "fps_confusion_s = {f'{k[0]} -> {k[1]}': v for k, v in confusion_info['confusion_map'].items()}\n",
    "\n",
    "show_confusion(y_gold, y_pred, labels, min_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('吉野', 'B-GOE_Other', 'B-Company'),\n",
       " ('吉野', 'B-GOE_Other', 'B-Company'),\n",
       " ('吉野', 'B-GOE_Other', 'B-Company'),\n",
       " ('トゥモロー', 'B-GOE_Other', 'B-Company'),\n",
       " ('ジャパレン', 'B-GOE_Other', 'B-Company'),\n",
       " ('ＳＯＧＯ', 'B-GOE_Other', 'B-Company'),\n",
       " ('大丸', 'B-GOE_Other', 'B-Company'),\n",
       " ('飯塚', 'B-GOE_Other', 'B-Company'),\n",
       " ('ラヂオ', 'B-GOE_Other', 'B-Company')]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fps_confusion_s['B-Company -> B-GOE_Other']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FP; 過抽出誤り"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select(list(fp_o['entry_map_pred'].items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
